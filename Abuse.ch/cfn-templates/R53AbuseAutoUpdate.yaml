Parameters:
  AbuseDLName:
    Description: "The name of the Domain List created to hold the rules"
    Type: String
    Default: 'AutoUpdating-Abuse-Hostfile'
  
Resources:
  AbuseDL:
    Type: AWS::Route53Resolver::FirewallDomainList
    Properties: 
      Name: !Sub ${AbuseDLName}
      Tags: 
        - Key: "ProjectName"
          Value: "R53-Abuse-AutoUpate"
        - Key: "downloaded-from"
          Value: "https://urlhaus.abuse.ch/downloads/hostfile/"
        - Key: "description"
          Value: "abuse.ch URLhaus Host file"
        - Key: "terms-of-use"
          Value: "https://urlhaus.abuse.ch/api/"
  S3Bucket:
    Type: AWS::S3::Bucket
    Properties:
      PublicAccessBlockConfiguration:
         BlockPublicAcls: true
         IgnorePublicAcls: true
         BlockPublicPolicy: true
         RestrictPublicBuckets: true
      BucketEncryption:
        ServerSideEncryptionConfiguration:
          - ServerSideEncryptionByDefault:
              SSEAlgorithm: 'AES256'
  S3BucketPolicy:
    Type: AWS::S3::BucketPolicy
    Properties: 
      Bucket: !Ref S3Bucket
      PolicyDocument:
        Statement:
          - Action:
              - 's3:GetObject'
              - 's3:PutObject'
              - 's3:DeleteObject'
            Effect: Allow
            Resource: !Sub "arn:aws:s3:::${S3Bucket}/autoupdating-dnsfirewall-domains.txt"
            Principal:
              AWS: !GetAtt LambdaExecutionRole.Arn
  LambdaExecutionRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
        - Effect: Allow
          Principal:
            Service:
            - lambda.amazonaws.com
          Action:
          - sts:AssumeRole
      Path: "/"
      Policies:
      - PolicyName: LambdaLogs
        PolicyDocument:
          Version: '2012-10-17'
          Statement:
          - Effect: Allow
            Action:
              - 'logs:CreateLogStream'
              - 'logs:PutLogEvents'
            Resource: !Sub 'arn:aws:logs:${AWS::Region}:${AWS::AccountId}:*'
      - PolicyName: R53Resolver
        PolicyDocument:
          Version: '2012-10-17'
          Statement:
          - Effect: Allow
            Action:
            - 'route53resolver:*'
            Resource: 
              - !GetAtt AbuseDL.Arn
      Tags: 
          - 
            Key: "ProjectName"
            Value: "R53-Abuse-AutoUpate"
  LambdaLogGroup:
    Type: AWS::Logs::LogGroup
    Properties:
      LogGroupName: !Sub "/aws/lambda/${LambdaFunction}"
      RetentionInDays: 14
    DeletionPolicy: Retain
  ScheduledRule: 
    Type: AWS::Events::Rule
    Properties: 
      Description: "AbuseDLDailyTrigger"
      ScheduleExpression: "cron(0 0 * * ? *)"
      State: "ENABLED"
      Targets: 
        - 
          Arn: 
            Fn::GetAtt: 
              - "LambdaFunction"
              - "Arn"
          Id: "TargetFunctionV1"
  PermissionForEventsToInvokeLambda: 
    Type: AWS::Lambda::Permission
    Properties: 
      FunctionName: !Ref "LambdaFunction"
      Action: "lambda:InvokeFunction"
      Principal: "events.amazonaws.com"
      SourceArn: 
        Fn::GetAtt: 
          - "ScheduledRule"
          - "Arn"
  LambdaInvoke:
    Type: AWS::CloudFormation::CustomResource
    DependsOn: S3BucketPolicy
    Version: "1.0"
    Properties:
      ServiceToken: !GetAtt LambdaFunction.Arn
  LambdaFunction:
    Type: AWS::Lambda::Function
    Properties:
      Role: !GetAtt LambdaExecutionRole.Arn
      Runtime: python3.13
      Handler: index.lambda_handler
      Timeout: 60
      Description: Used to fetch data from the abuse.ch rule list and update the associated Domain List
      Tags: 
        - 
          Key: "ProjectName"
          Value: "R53-Abuse-AutoUpate"
      Code:
        ZipFile: !Sub |
          import boto3
          import json
          import urllib.request
          import re
          from botocore.exceptions import ClientError
          import cfnresponse

          s3 = boto3.client('s3')
          route53resolver = boto3.client('route53resolver')

          HOSTFILE_URL = "https://urlhaus.abuse.ch/downloads/hostfile/"
          BUCKET_NAME = "${S3Bucket}"
          DOMAIN_LIST_ID = "${AbuseDL}"
          FILE_KEY = "autoupdating-dnsfirewall-domains.txt"

          # Compile the pattern once
          DOMAIN_PATTERN = re.compile(
              r'^'                           # Start of string
              r'(?!-)'                       # Cannot start with hyphen
              r'(?!.*--)'                    # No consecutive hyphens
              r'(?=.{1,253}$)'              # Total length 1-253 (RFC compliant)
              r'(?:'                         # Non-capturing group for labels
                  r'[a-zA-Z0-9]'             # Label must start with alphanumeric
                  r'(?:[a-zA-Z0-9-]{0,61}'   # Middle can have hyphens (max 63 total)
                  r'[a-zA-Z0-9])?'           # Must end with alphanumeric if >1 char
                  r'\.'                      # Followed by dot
              r')+'                          # One or more labels
              r'[a-zA-Z]'                    # TLD starts with letter
              r'(?:[a-zA-Z0-9-]{0,61}'       # TLD middle part
              r'[a-zA-Z0-9])?'               # TLD ends with alphanumeric if >1 char
              r'$'                           # End of string
          )

          def import_list(s3_obj):
              try:
                  res = route53resolver.import_firewall_domains(
                      DomainFileUrl=s3_obj,
                      FirewallDomainListId=DOMAIN_LIST_ID,
                      Operation="REPLACE"
                  )
                  print(f"Updating the domain list using {s3_obj}")
                  return res
              except ClientError as e:
                  print(f"Error importing firewall domains: {e}")
                  return None

          def write_to_s3(domains):
              try:
                  s3.put_object(
                      Bucket=BUCKET_NAME,
                      Key=FILE_KEY,
                      Body='\n'.join(domains),
                      ContentType="text/plain"
                  )
                  return f"s3://{BUCKET_NAME}/{FILE_KEY}"
              except ClientError as e:
                  print(f"Error writing to S3: {e}")
                  return None

          def delete_from_s3():
              try:
                  s3.delete_object(Bucket=BUCKET_NAME, Key=FILE_KEY)
                  print("Deleted the import file")
              except ClientError as e:
                  print(f"Error deleting from S3: {e}")

          def parse_hostfile(content):
              valid_domains = {
                  parts[1].strip()                                    # Extract and clean the domain name
                  for line in content.splitlines()                    # Split content into lines
                  if not line.startswith('#') and line.strip()        # Skip comments and empty lines
                  if (parts := line.split('\t')) and len(parts) >= 2  # Split by tab and ensure we have at least 2 parts
                  if DOMAIN_PATTERN.match(parts[1].strip())           # Validate domain with regex pattern
              }
              
              return valid_domains

          def get_domains():
              print(f"Fetching the list of domains from {HOSTFILE_URL}")
              try:
                  with urllib.request.urlopen(HOSTFILE_URL) as response:
                      hostfile_content = response.read().decode('utf-8')
                  
                  valid_domains = parse_hostfile(hostfile_content)
                  print(f"Fetched {len(valid_domains)} domains")
                  return list(valid_domains)  # Convert back to list for consistency
              except urllib.error.URLError as e:
                  print(f"Error fetching domains: {e}")
                  return []
              
          def lambda_handler(event, context):
              try:
                  if event.get('RequestType') == "Delete":
                      delete_from_s3()
                      cfnresponse.send(event, context, cfnresponse.SUCCESS, {})
                      return {'statusCode': 200, 'body': json.dumps('SUCCESS')}

                  domains = get_domains()
                  if not domains:
                      print("ERROR - Unable to fetch domain list")
                      cfnresponse.send(event, context, cfnresponse.FAILED, {}, reason="Unable to fetch domain list")
                      return {'statusCode': 500, 'body': json.dumps('FAILED - Unable to fetch domain list')}

                  s3_obj = write_to_s3(domains)
                  if not s3_obj:
                      print("ERROR - Unable to write domain file to S3")
                      cfnresponse.send(event, context, cfnresponse.FAILED, {}, reason="Unable to write domain file to S3")
                      return {'statusCode': 500, 'body': json.dumps('FAILED - Unable to write domain file to S3')}

                  res = import_list(s3_obj)
                  if res:
                      print("Done")
                      cfnresponse.send(event, context, cfnresponse.SUCCESS, {})
                      return {'statusCode': 200, 'body': json.dumps('SUCCESS')}
                  else:
                      print("ERROR - Import failed")
                      cfnresponse.send(event, context, cfnresponse.FAILED, {}, reason="Import failed")
                      return {'statusCode': 500, 'body': json.dumps('FAILED - Import failed')}
              except Exception as e:
                  print(f"ERROR: {str(e)}")
                  cfnresponse.send(event, context, cfnresponse.FAILED, {}, reason=str(e))
                  return {'statusCode': 500, 'body': json.dumps(str(e))}